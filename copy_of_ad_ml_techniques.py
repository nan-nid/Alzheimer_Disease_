# -*- coding: utf-8 -*-
"""Copy of AD_ML techniques.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TYqDCZuKyPDSHz8890sFHzATHkenk5Jf
"""

import os
import pandas as pd
import numpy as np
from matplotlib import image

import seaborn as sns
import matplotlib.pyplot as plt

from skimage.transform import resize
from skimage.io import imread
from skimage.color import rgb2gray

from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold


from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report



sns.set_palette('Paired')
sns.set_style("whitegrid")

from google.colab import drive
drive.mount('/content/drive')

# class1 = 'Very_Mild_Demented/'
# class2 = 'Mild_Demented/'
# class3 = 'Moderate_Demented/'
# class0 = 'Non_Demented/'


class1 = '/content/drive/MyDrive/Dataset/Very_Mild_Demented'
class2 = '/content/drive/MyDrive/Dataset/Mild_Demented'
class3 = '/content/drive/MyDrive/Dataset/Moderate_Demented'
class0 = '/content/drive/MyDrive/Dataset/Non_Demented'

"""### Step 1 : Data Extraction / Cleaning / Aggregation

1. In this step, data is extracted from each source, cleaned, resized, flattened and finally combined with all the sources to form an aggreagated dataframe that consists of all the classes.
"""

def file_append(class_path):
    image_array = []
    curr_path = os.path.join(os.getcwd(),class_path)
    cnt = 0
    file_list = [k for k in os.listdir(curr_path) if '.jpg' in k]
    for x in file_list:

        img_path = os.path.join(curr_path,x)

        img = imread(img_path)
#         img = rgb2gray(img)
        img = resize(img,(50,50,5))     ## Will need to change this resize parameter
        img = img.flatten()
        image_array.append(img)

    return image_array


class1_img = file_append(class1)
class2_img = file_append(class2)
class3_img = file_append(class3)
class0_img = file_append(class0)

df1 = pd.DataFrame(class1_img)
df1['y']  = 1
df2 = pd.DataFrame(class2_img)
df2['y']  = 2
df3 = pd.DataFrame(class3_img)
df3['y']  = 3
df0 = pd.DataFrame(class0_img)
df0['y']  = 0

df = pd.concat([df0,df1,df2,df3],ignore_index=True)
df = df.sample(frac=1, random_state=50).reset_index(drop=True)

df.head()

"""### Step 1.1 : Checking for null values"""

df.isnull().sum().sum()

"""## Step 2 : EDA of variable"""

sns.barplot(x =['Non-dementia','Very Mild', 'Mild', 'Moderate'],y = df['y'].value_counts())
plt.ylabel('Count')
plt.title('Class Wise Distribution of data')

"""This distribution makes sense, as the percentage of people who have severe cases is much lower than mild cases.

## Step 3 : Splitting entire dataset into train and test sets
"""

X,y = df[df.columns[:-1]],df['y']
X_train,X_test,y_train,y_test = train_test_split(X,y,train_size=0.9,random_state=130)

"""## Step 4 : Data Preprocessing pipeline for scaling and dimensionality reduction"""

data_pipe = Pipeline([('scaler', StandardScaler()), ('pca',PCA(n_components=0.92))])
data_pipe.fit(X_train)

X_train = data_pipe.transform(X_train)
X_test = data_pipe.transform(X_test)

y_train

# pca = PCA(n_components=0.90)
# pca_data = pca.fit_transform(X_train)
# exp_var = pca.explained_variance_ratio_

print(f"Original Data dimension: {df.shape[1]} features")
print(f"After PCA Data dimension: {X_train.shape[1]} features")

top10_var = [x*100 for x in data_pipe.named_steps['pca'].explained_variance_ratio_[:10]]
PC_labels = ['PC_'+str(x+1) for x in range(len(top10_var)) ]

plt.figure(figsize=(8,6))
plt.title("Variance Percentage for top 10 Principal Components")

bar = sns.barplot(x = top10_var,y = PC_labels)
for p in bar.patches:
    x = p.get_width()
    y = p.get_y()
    bar.annotate(format(x,'.1f'),(x,y),ha = 'center', va = 'center',size=12,color='black',xytext = (0, -20), textcoords = 'offset points',bbox = dict(boxstyle = 'square, pad = 0.2', lw = 0.8, ec = '#6e6e6e'))
#     print(p.get_width())

plt.title("Variance Ratio for top 10 Principal Components")

scatter_plots = X_train[:,:8]
data_cols = ['PC_'+str(x+1) for x in range(scatter_plots.shape[1]) ]
scatter_df = pd.DataFrame(scatter_plots,columns=data_cols)
scatter_df['y'] = y_train.reset_index(drop=True)

label_map = {0:'No Dementia',1:'Very Mild', 2: 'Mild',3:'Moderate-High'}
scatter_df['y'] = scatter_df['y'].map(label_map)

sns.pairplot(scatter_df,hue="y",palette="husl",)

"""From the above plot, it is very clear that , the data is not separable in lower dimensions, as we can see in scatter plots. Hence it is not easy to find a linearly separable boundary, thus it is my assumption that we might need to opt for SVM or other methods for high accuracy.

## Step 5 : Data Modelling
"""

#Base function for models

def train_model(model,search_grid):
    cv = KFold(n_splits=10)
    grid_search = GridSearchCV(estimator=model, param_grid=search_grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)
    grid_result = grid_search.fit(X_train,y_train)

    return grid_result

def scatter_plots(score_array,title):
    sns.set_palette('CMRmap')
    lr_scores = [max(score_array.cv_results_['split'+str(i)+'_test_score']) for i in range(0,10)]
    sns.lineplot(range(1,11),lr_scores)
    j = plt.scatter(range(1,11),lr_scores)

    plt.xlabel('Cross Validation Steps')
    plt.ylabel('Model Accuracy')
    plt.title(f'Cross Validation Accuracies for {title}',size=15)

    for i,j in enumerate(lr_scores):
        if j == max(lr_scores):
            plt.scatter(i+1,j,c='red',marker='o',s=200)
            plt.annotate(format(j,'.3f'),(i+1,j),size=15)

def cf_plot(score_array,title):
    plt.figure(figsize=(9,7))
    cf_matrix = confusion_matrix(y_test,y_pred=score_array.predict(X_test))
#     sns.heatmap(cf_matrix/np.sum(cf_matrix,axis=0), annot=True, fmt='.2%', cmap='Blues',annot_kws={"size":15})

    sns.heatmap(cf_matrix, annot=True, fmt='g', cmap='Blues',annot_kws={"size":15})

    plt.xlabel('Predicted Label',size=15)
    plt.ylabel('Actual Label',size=15)
    plt.title(f'Confusion Matrix for {title}',size=15)
    print(classification_report(y_test,y_pred=score_array.predict(X_test)))

"""### Step 5.2 : SVM"""

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC

# Create an SVM classifier
clf = SVC()

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the classifier performance
accuracy = np.mean(y_pred == y_test)
print('Accuracy:', accuracy)

import numpy as np
from sklearn.metrics import confusion_matrix
from sklearn.svm import SVC

def create_confusion_matrix(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  return conf_mat

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Create the confusion matrix
conf_mat = create_confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print(conf_mat)

"""**KNN**"""

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
# Create a KNN classifier
knn = KNeighborsClassifier(n_neighbors=5)

# Train the classifier
knn.fit(X_train, y_train)

# Make predictions on the test set
y_pred = knn.predict(X_test)

# Evaluate the classifier performance
accuracy = np.mean(y_pred == y_test)
print('Accuracy:', accuracy)

import numpy as np
from sklearn.metrics import confusion_matrix

def create_confusion_matrix(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  return conf_mat
# Load the training and testing data
# ...
# Create the KNN classifier
from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier(n_neighbors=5)
# Train the model
knn.fit(X_train, y_train)
# Make predictions on the test data
y_pred = knn.predict(X_test)
# Create the confusion matrix
conf_mat = create_confusion_matrix(y_test, y_pred)
# Print the confusion matrix
print(conf_mat)
def create_confusion_matrix_grid(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  df_cm = pd.DataFrame(conf_mat, index=["Actual Positive", "Actual Negative"],
                     columns=["Predicted Positive", "Predicted Negative"])
  return df_cm
# Create the confusion matrix grid
conf_mat_grid = create_confusion_matrix_grid(y_test, y_pred)

# Print the confusion matrix grid
print(conf_mat_grid.to_string())

"""**DECISION** **TREE**"""

from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
# Create a decision tree classifier
clf = DecisionTreeClassifier()

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the classifier performance
accuracy = np.mean(y_pred == y_test)
print('Accuracy:', accuracy)

def create_confusion_matrix(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  return conf_mat
# Create the decision tree classifier
from sklearn.tree import DecisionTreeClassifier
dtree = DecisionTreeClassifier()

# Train the model
dtree.fit(X_train, y_train)

# Make predictions on the test data
y_pred = dtree.predict(X_test)

# Create the confusion matrix
conf_mat = create_confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print(conf_mat)

cm = confusion_matrix(labels, predictions)
cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)
cm_df
plt.figure(figsize=(10,6), dpi=400)
sns.heatmap(cm_df, annot=True, cmap="Greys", fmt=".1f")
plt.title("Confusion Matrix", fontweight="bold")
plt.xlabel("Predicted", fontweight="bold")
plt.ylabel("True", fontweight="bold")

"""**RANDOM FOREST**"""

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
# Create a random forest classifier
clf = RandomForestClassifier(n_estimators=100)

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the classifier performance
accuracy = np.mean(y_pred == y_test)
print('Accuracy:', accuracy)

def create_confusion_matrix(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  return conf_mat

# Create the random forest classifier
clf = RandomForestClassifier()
# Train the model
clf.fit(X_train, y_train)
# Make predictions on the test data
y_pred = clf.predict(X_test)
# Create the confusion matrix
conf_mat = create_confusion_matrix(y_test, y_pred)
# Print the confusion matrix
print(conf_mat)

"""**XG Boost**"""

from sklearn.model_selection import train_test_split
import xgboost as xgb

# Create an XGBoost classifier
params = {'n_estimators': 100, 'learning_rate': 0.1, 'max_depth': 5}
clf = xgb.XGBClassifier(**params)

# Train the classifier
clf.fit(X_train, y_train)

# Make predictions on the test set
y_pred = clf.predict(X_test)

# Evaluate the classifier performance
accuracy = np.mean(y_pred == y_test)
print('Accuracy:', accuracy)

def create_confusion_matrix(y_true, y_pred):
  conf_mat = confusion_matrix(y_true, y_pred)
  return conf_mat

# Make predictions on the test data
y_pred = clf.predict(X_test)

# Create the confusion matrix
conf_mat = create_confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print(conf_mat)

"""**Graphical Represenataion for Accuracy Comaprison of Various Model**"""

import matplotlib.pyplot as plt
import numpy as np

# Define the accuracy of each model
accuracy = {'CNN': 0.985, 'SVM': 0.946, 'KNN': 0.971, 'DT': 0.585, 'RF': 0.701, 'XGB': 0.804}

# Create the bar chart
plt.bar(accuracy.keys(), accuracy.values())
plt.bar(accuracy.keys(), accuracy.values(), color=['red', 'green', 'blue', 'yellow', 'purple','black'], width=0.8)
plt.xlabel("Model")
plt.ylabel("Accuracy")
plt.title("Accuracy of Machine Learning Models")
plt.show()